\chapter{Instantiating a cognitive model for driving context classification}
\begin{itemize}
	\item example tasks
	\item data sets
	\item labelling/training data generation
	\item implementation in \ac{Nengo}
	\item implementation details
\end{itemize}
\label{sec:driving_context_class}
Fig. \ref{subfig:system_arch} shows a schematic overview of our system architecture.
\textit{Environment perception} happens through a variety of different sensors \cite{Aeberhard2015} providing \textit{preprocessed data} in the form of object-lists or raw sensory data.
We build our representation from this preprocessed data through \textit{vectorization} of sensory data and high-level object lists.
We use a \textit{neural network} for predictions based on the current scene vector.
Here, we use supervised learning to distinguish three different driving contexts, namely city, interurban and highway.
\section{Training data}
The input data used for training and evaluating the system is real-world data gathered during test drives in the region of Munich, Germany.
Depending on the test vehicle's sensor setup \cite{Aeberhard2015}, a subset of the following sensor systems is available: camera, RADAR, LIDAR as well as the dynamics of the ego-vehicle (e.g. velocity, acceleration, steering angle) through introspective sensors.
While lists of dynamic objects (i.e. cars, pedestrians, etc.) are available from all extrospective sensors, the camera-based perception system additionally provides lists of static objects like traffic signs.
In this work, we focus on the ego-vehicle's dynamics and the information provided by the camera-system as the only extrospective sensor.
The camera-system is present in all available test traces and furthermore, its data is most informative regarding categories of dynamic objects while being the only system that provides information about traffic signs.
Beside the object's classification, the camera systems provides estimations (with variance) of entities like relative position, orientation and velocity for each object.
The data is divided into three different sets: one for training and two test sets containing roughly \SIlist{27;18;7}{\minute} respectively of driving data.\\
To enable automated training of any supervised learning system, the training data needs to be labeled.
In this section, we hand-labeled our data sets by visually inspecting the images provided by a reference camera-system and labelling the intervals  between transitions of driving contexts as indicated by the respective traffic signs.
\section{Scene representation in vectors}
\begin{figure}[t!]
	\centering
	\subfloat[Example scene with labeled object vectors based on environment perception\label{subfig:ex_scene}]{%
		\includegraphics[width=0.45\textwidth, height=3.2cm]{imgs/Example_scene.eps}
	}
	\subfloat[System architecture\label{subfig:system_arch}]{%
		\includegraphics[width=0.45\textwidth, height=3.2cm]{imgs/system_overview_horizontal.eps}
	}
	\caption{Schematic system overview with one example scene.}\label{fig:sys}
\end{figure}
For this application, we encapsulate three types of information in our vector-based scene representation: ego-vehicle dynamics , dynamic objects and traffic signs (provided as preprocessed object-lists).
For each category, we describe the process of converting the input data into a vector representation.
We obtain the final vector describing the current scene by superposition of all vectors created in each category.
For all our vectors in this work we have chosen a dimension of $D=512$, a reasonable trade-off between informational capacity and computational complexity.\\
%\subsubsection{Ego-vehicle dynamics}
%\label{subsubsec:ego-veh-dyn}
\subsection{Ego-vehicle dynamics}
For ego-vehicle dynamics, we use the current velocity, acceleration in $x$/$y$-direction (ego-vehicle coordinate system), the angle of the steering wheel as well as the steering angle of the front axle.
For all values except acceleration, we randomly choose one normalized ID-vector representing the respective value, e.g. $\mathbf{VELOCITY}=\left(v_1, \cdots, v_D\right)$ with $v_i \in \mathbb{R}$ for velocity, and multiply this ID-vector with the current scalar value $x$, e.g. $x \cdot \mathbf{VELOCITY}$ for velocity.
Furthermore, we normalize all scalar values to the range $\left[-2,2\right]$ to keep the length of our vectors limited.
For vectorization of two-dimensional values, we use an encoding with sine and cosine functions with different spatial frequencies and offsets.
Therefore, we define the following helper functions
\[ \abb{f_{\left(m,i\right)}}{\mathbb{R}^2}{\mathbb{R}^4}{\left(x,y\right)}{\left(\cos\frac{m\cdot \pi + x}{i + 1}, \sin\frac{m\cdot \pi + x}{i + 1}, \cos\frac{m\cdot \pi + y}{i + 1}, \sin\frac{m\cdot \pi + y}{i + 1}\right)},
\]
\[
\abb{\psi_i}{\mathbb{R}^2}{\mathbb{R}^4}{\left(x,y\right)}{\left(f_{\left(0,i\right)}\left(x,y\right), f_{\left(\frac{1}{2},i\right)}\left(x,y\right), f_{\left(1,i\right)}\left(x,y\right), f_{\left(\frac{3}{2},i\right)}\left(x,y\right)\right)}
\]
and obtain the final vector representation of acceleration in $x$/$y$-direction via the function
\[
\abb{\lambda}{\mathbb{R}^2}{\mathbb{R}^D}{\left(x,y\right)}{\frac{1}{\sqrt{\frac{D}{2}}}\left(\psi_0\left(x,y\right), \cdots, \psi_{\frac{D}{16}-1}\left(x,y\right)\right).}
\]
This encoding $\lambda\left(a_x, a_y\right)$ leads to normalized, nonzero, similar vectors with information distributed over all elements (in contrast to a simple encoding like $\left(a_x, a_y, 0 \cdots, 0\right)$).\\
%\subsubsection{Dynamic Objects}
\subsection{Dynamic objects}
The camera-based classification system is able to distinguish seven different object categories, namely bicycle, car, motorcycle, pedestrian, stationary, truck and unknown.
In the simplest form of our vector representation, we assign one random vector to each of those categories and add it to the current scene representation once for each category's occurrence in the object-list.
However, this representation just encodes that there are certain objects present somewhere in the current scene without any additional information.
Enhancing this simple encoding, we use the function $\lambda$ to map each dynamic object's position in $x$/$y$-direction (relative to the ego-vehicle) to vector form and bind the result to the vector representing the object's category.
%However, position information might not be that informative regarding the task of distinguishing between driving contexts.
One quite unique feature of e.g. highway driving is the fact that almost all other traffic participants drive in similar direction as the ego-vehicle.
Therefore, we create additional random vectors encoding the orientation of dynamic objects relative to the ego-vehicle for three discretized categories, namely $\mathbf{SAME}$, $\mathbf{OPPOSITE}$ and $\mathbf{LATERAL}$.
%We can now bind this orientation information to each dynamic object as we did for position information.
If we want to jointly bind those two pieces of information to one object, we need to introduce two additional ID-vectors $\mathbf{POSITION}$ and $\mathbf{ORIENTATION }$ to impose structure.
For example, a car detected at position $\left(p_x,p_y\right)$ with approximately the same orientation as the ego-vehicle would lead to the following vector representation
\[
\mathbf{CAR} + \mathbf{CAR}\otimes\mathbf{POSITION}\otimes\lambda\left(p_x,p_y\right) + \mathbf{CAR}\otimes\mathbf{ORIENTATION}\otimes\mathbf{SAME}.
\]
%\subsubsection{Traffic Signs}
\subsection{Traffic signs}
The ego-vehicle's camera-system \cite{Aeberhard2015} is able to recognize a significant amount and variety of traffic signs.
Again, we assign a random vector to each possible traffic sign label and add it to the current scene representation.
However, in contrast to dynamic objects, most traffic signs are not only valid while being visible but stay relevant for the current driving context until withdrawn by another sign.
Therefore, we implemented a simple form of memory for a certain subset of traffic signs relevant to the task of driving context classification even after disappearing. %, namely speed limit signs as well as signs indicating "environmental changes" like e.g. entrance/exit of a city or highway.
Due to the fact, that the camera system is not immune to false detections, we implemented a decaying memory, to avoid relying too much on false detections and to allow the system to consider other cues.
We realized the decay by multiplying each vector representing a traffic sign with a (parameterizable) scalar decay factor  at each time step.
Furthermore, we included a simple withdraw logic, e.g. new speed limit signs overwrites previously seen ones and a sign indicating a city entrance withdraws a memorized highway sign.\\
%\subsection{Training}
\section{Experimental results}

\subsection{Baseline for classification: human level performance}
To get a better understanding of the quality of the context classification system's results, we compare it to human level performance.
However, neither showing raw camera images nor numerical vectors would yield comparable results.
Therefore, we created human-readable versions of our input vectors in text form and presented a subset of $50$ random samples for each data set (with the training set always being the first) to two human subjects asking for their classification guess.
