\chapter{Theoretical Background}
In this chapter, we briefly present the theoretical background for some frameworks and methods used throughout this thesis.


\section{\aclp{VSA}}
The term \acp{VSA} - first coined by Ross Gayler \cite{Gayler2003} - refers to a class of similar approaches for cognitive modeling making use of distributed representations.
The basic idea behind all of those approaches is to represent structure (e.g. cognitive concepts, symbols or language) in a high-dimensional vector space by mapping each entity to be represented to a (possibly random) vector.
One of the most important properties of high-dimensional vector spaces enabling this kind of representation is the fact, that two high-dimensional random vectors are likely to be dissimilar.
In the following, we will show what we mean by fuzzy terms like \emph{dissimilar} or \emph{likely} and provide more precise statements.\\
One main requirement in the context of cognitive modeling is the ability of the modeling framework to address the binding problem.
In \cite{Jackendoff2002}, Jackendoff phrases this as the problem of "combining independent bits into a single coherent percept". 
One strength of \acp{VSA} is that they offer the possibility to manipulate their entities (i.e. vectors) through algebraic operations, usually at least one \emph{addition-like} and \emph{multiplication-like} operation each.
Typically, the multiplication operation is used for binding different representations into a new vector.
This operation, depending on the vector representation, is constructed with some desirable properties in mind (see section ??\todo{insert reference}) %TODO insert reference.
A first attempt on using a multiplication operation on vectors for binding was done by Smolensky \cite{Smolensky1990} using the tensor product.
The major drawback of this approach is exploding dimensionality of the tensor product.
For finite dimensional vector spaces  $V$ and $W$ of dimensions $n$ and $m$, the tensor space $V \otimes W$ is a vector space of dimension $n\cdot m$.
As a consequence, each binding operation $v\otimes w$ for vectors $v \in V, w \in W$ would increase the dimension of the representational space, which is computationally infeasible and leads to poor scaling.
This lead researchers to define several slightly different multiplication or binding operations, depending on the underlying numerical structure.
The most prominent examples are elementwise multiplication in Gayler's \ac{MAP}-architecture \cite{Gayler1998}, the XOR-operation in Kanerva's \acp{BSC} \cite{Kanerva2000, Kanerva2009} as well as circular convolution in Plate's \acp{HRR} \cite{Plate1991, Plate1994}.

\subsection{Mathematical properties of \aclp{VSA}}
%Before we provide a formal definition for \acp{VSA}, we introduce some terms and auxiliary tools needed for later use.
%\begin{defn}
%	\label{def:metric}
%	Let $M$ be a set. A function 
%	\[
%	\abb{d}{M \times M}{\mathbb{R}}{(x,y)}{d(x,y)} 
%	\]
%	is called a \emph{metric}, if and only if for any $x, y \in M$ the following conditions hold:
%	\begin{enumerate}
%		\item $d(x,y) \geq 0$ (non-negativity)
%		\item $d(x,y) = 0 \Longleftrightarrow x = y$ (identity of indiscernibles)
%		\item $d(x,y) = d(y,x)$ (symmetry)
%		\item $d(x,z) \leq d(x,y) + d(y,z)$ (triangle inequality)
%	\end{enumerate}
%	We call the ordered pair $(M,d)$ a \emph{metric space}.
%\end{defn}
 

\begin{defn}
	\label{def:VSA}
	Let $N$ be a set of numbers and $D \in \mathbb{N}$ a natural number. 
	Furthermore, let 
	\[V_{D}(N)=\{\left(x_{0}, \cdots, x_{D-1}\right)  | x_{i} \in N\}\] 
	be the set of all $D$-tuples with entries in $N$. 
	Let
	\begin{align*}
		&\abb{\oplus}{V_{D}(N) \times V_{D}(N)}{V_{D}(N)}{(v,w)}{\oplus(v,w) =: v\oplus w}, \\
		&\abb{\varoast}{V_{D}(N) \times V_{D}(N)}{V_{D}(N)}{(v,w)}{\varoast(v,w) =: v\varoast w}
	\end{align*}
	be functions with $\oplus$ following the rules of ordinary addition - namely commutativity, associativity, existence of a neutral element and existence of inverse elements - and for any elements $u,v,w \in V_{D}(N)$
	\[u \varoast (v \oplus w) = u \varoast v \oplus u \varoast w.\]
	If there is furthermore a distinct element $\pmb{1} \in V_{D}(N)$ with 
	\[v \varoast \pmb{1} = \pmb{1} \varoast v = v\]
	for any $v \in V_{D}(N)$ and a function $\abbil{\phi}{V_{D}(N) \times V_{D}(N)}{\left[-1,1\right]}$, we call $(V_{D}(N), \varoast, \oplus, \phi)$ a \emph{\acrfull{VSA}} of dimension $D$.
	The function $\phi$ is called a \emph{measure of similarity}.
	If $N$ is a subset of the real or complex numbers, i.e. $N \subset \mathbb{R}$ or $N \subseteq \mathbb{C}$, we call any \ac{VSA} $\left(V_{D}(N), \varoast, \oplus, \phi\right)$ \emph{continuous}.
\end{defn}
Although the set $V_{D}(N)$ might not be a vector space in the strict mathematical sense (in most cases it is at least a subset of a vector space), we will refer to its elements as \emph{vectors}.
%The metric in definition \ref{def:VSA} is needed as measure of similarity between vectors. 
Before we proceed in deriving some important properties of \acp{VSA}, we present some of the most prominent examples.

\begin{ex} \aclp{VSA}
	\begin{enumerate}
		\item The first example of a \ac{VSA} is Kanerva's \acfl{BSC} \cite{Kanerva2009}. 
		He restricts the elements of his vectors to binary values, i.e. $N=\{0,1\}$
		The operations $\varoast$ and $\oplus$ in this case are the XOR-function and a thresholded sum respectively.
		With $v_{i} = \left(v_{i 0}, \cdots, v_{i D-1}\right) \in V_{D}(N)$ and  $i \in \{1, \cdots, n\}$, the operation $\oplus$ is usually defined in the following way
		\begin{align*}
		v_{1} \oplus \cdots \oplus v_{n} =: &x = \left(x_{0}, \cdots, x_{D-1}\right) \textrm{ with } \\
		&x_{j}:= \begin{cases}
		1 & \sum\limits_{i=1}^{n} v_{ij} \geq \frac{n}{2} \\
		0 & \sum\limits_{i=1}^{n} v_{ij} < \frac{n}{2}
		\end{cases}.
		\end{align*}
		This definition ensures, that the results of the addition operation $\oplus$ remain binary.
		Usually, a normalized Hamming distance 
		\[
		\phi(v,w) := 1 - \frac{2}{D} \left| \{ v_{i} \neq w_{i} | i \in \{0, \cdots, D-1\} \} \right|
		\]
		is used as a measure of similarity in this architecture.
		\acp{BSC} have some interesting properties compared to other \acp{VSA}: 
		The neutral element for both operations $\varoast$ and $\oplus$ is the vector $\pmb{0} := \left(0, \cdots, 0\right)$, while all vectors are self-inverse regarding the multiplication operation $\varoast$, i.e. $v \varoast v = \pmb{0}$ for any $v \in V_{D}(N)$.
		
		\item The first example of \ac{VSA} in continuous space is Gayler's \acrfull{MAP} architecture \cite{Gayler1998} with $N \subseteq \mathbb{R}$ and the cosine simlarity as measure of similarity 
		\[
		\phi(v,w) = \frac{v \cdot w}{\norm{v}\norm{w}}=\cos(\theta),
		\]
		with $\theta$ being the angle between the vectors $v,w \in V_{D}(N)$.
		The operations $\varoast$ and $\oplus$ are simply element-wise multiplication and addition with neutral elements $\pmb{1}=\left(1, \cdots, 1\right)$ and $\pmb{0} := \left(0, \cdots, 0\right)$ respectively.
		
		\item Another example of a \ac{VSA} in continuous space is Plate's \acfl{HRR} \cite{Plate1994, Plate1997}.
		The main difference compared to the \ac{MAP} architecture is, that Plate in general allows complex vector values, i.e. $N \subseteq \mathbb{C}$ and uses a different multiplication operation $\varoast$ - namely circular convolution.
		For any two vectors $v, w \in V_{D}(N)$, circular convolution $\varoast$ ist defined as
		\begin{align*}
		z = v \varoast w \qquad \textrm{ with } z_{j} := \sum_{k=0}^{D-1} v_{k}w_{(j-k)\Mod{D}}.
		\end{align*}
		\todo{include visualization of circular convolution}
		The neutral element regarding circular convolution is $\pmb{1} = \left(1, 0, \cdots, 0\right)$. 
		One important property of this operation is the fact, that circular convolution can efficiently be computed using the \ac{DFT}.
		The \ac{DFT} is defined as the function
		\[
		\abb{\ac{DFT}}{\mathbb{C}^{D}}{\mathbb{C}^{D}}{x}{\left(\sum_{j=0}^{D-1} x_{j} \zeta_{D}^{-jk} \right)_{k=0}^{D-1}} \qquad \textrm{ with } \zeta_{D} = \exp\left( \frac{i 2 \pi}{D} \right).
		\]
		Similarly, the \ac{IDFT} is defined as the function 
		\[
		\abb{\ac{IDFT}}{\mathbb{C}^{D}}{\mathbb{C}^{D}}{x}{\left( \frac{1}{D} \sum_{j=0}^{D-1} x_{j} \zeta_{D}^{jk} \right)_{k=0}^{D-1}}.
		\]
		From the convolution theorem we know, that we can calculate the circular convolution of any two vectors $v, w \in V_{D}(N)$ by
		\[
		v \varoast w = \ac{IDFT}\left(\ac{DFT}(v) \odot \ac{DFT}(w) \right),
		\]
		with $\odot$ denoting element-wise multiplication in this case.
		This induces that circular convolution obeys the same rules (commutativity and associativity) as element-wise multiplication, as both operations are the same except for a change of basis. 
	\end{enumerate}
\end{ex}
As mentioned earlier, one of the most important features of (high-dimensional) \acp{VSA} is the fact that two random vectors are likely to be dissimilar.
We will derive this result in the following theorem.
\begin{theorem}
	Let $\left(V_{D}(N), \varoast, \oplus, \phi \right)$ a \acl{VSA}. 
	For two randomly chosen vectors $v, w \in V_{D}(N)$, the distribution of the similarity $\phi\left(v,w\right)$ is a version of the beta-distribution $\beta\left(\frac{D-1}{2},\frac{D-1}{2}\right)$ scaled and shifted to the interval $\left[-1,1\right]$ with mean $\mu=0$ and variance $\sigma^2=\frac{c^2}{D}$ up to a constant $c$. The standardized distribution trends with growing $D$ to a normal distribution.
\end{theorem}
\begin{proof}
	We will only give the proof of this theorem for real valued \acp{VSA}, i.e. $N \subseteq \mathbb{R}$ and $\phi$ as the cosine similarity.
	Without loss of generality, we assume the vectors $v,w$ picked randomly from the unit sphere $\mathbb{S}^{D-1} = \{ v \in \mathbb{R}^{D} | \norm{v} = 1 \}$, as we can simply normalize the vectors by $\frac{v}{\norm{v}}$.
	Since binary \acp{VSA} can be associated with a euclidean sphere as well, the same result can be proven for those architectures with similar arguments (see \cite{Kanerva1988} for details).
	Due to symmetry of the unit sphere $\mathbb{S}^{D-1}$, we can furthermore - again without loss of generality - choose one vector as a unit vector, i.e. $w=\left(1, 0 , \cdots, 0\right)$.
	Thereby, the cosine similarity for $v=\left(v_{0}, \cdots, v_{D-1}\right)$ is given by $\phi\left(v,w\right) = v_{0}$
	By fixing one coordinate, we get the constraint $\sum_{i=1}^{D-1} v_{i}^{2} = 1-v_{0}^{2}$ which is equivalent to a lower dimensional sphere $\mathbb{S}^{D-2}$ with radius $\sqrt{1-v_{0}^2}$.
	Hence the cosine similarity $\phi\left(v,w\right)=:x$ is proportional to the surface of a conical frustum constructed from $\mathbb{S}^{D-2}$ with radius $\sqrt{1-x^{2}}$, slope $\frac{1}{\sqrt{1-x^{2}}}$ and some height $h$, i.e. the density function is proportional to
	\[
	f_{\phi(v,w)}(x) \propto \frac{\sqrt{1-x^{2}}^{(D-2)}}{\sqrt{1-x^{2}}} h \propto \left(1-x^{2}\right)^{\frac{D-3}{2}}.
	\]
	Substituting $x=2u-1$, we get 
	\[
	\left(1-\left(2u-1\right)^{2}\right)^{\frac{D-3}{2}} \propto \left(u-u^2\right)^{\frac{D-3}{2}} = \left(u \left(1-u\right)\right)^{\frac{D-3}{2}} = u^{\left(\frac{D-1}{2}-1\right)} \left(1-u\right)^{\left(\frac{D-1}{2}-1\right)},
	\]
	which is the density function of the beta distribution $\beta\left(\frac{D-1}{2},\frac{D-1}{2}\right)$.
	Thus, the cosine similarity is also beta distributed, but scaled and shifted to the interval $\left[-1,1\right]$ by $x=2u-1$.\\
	For $\alpha=\beta=\frac{D-1}{2}$, the mean of the beta distribution is $\tilde{\mu}=\frac{1}{2}$. Applying the substitution, we get the mean of the shifted distribution $\mu = 2\tilde{\mu }-1 = 0$.\\
	Making use of the simplification that the distribution of similarity is the same as the distribution in the first coordinate, the variance is given by the expected value of the square value of the first coordinate, i.e. $\mathbb{E}(v_{0}^{2})$.
	Since all coordinate are identically distributed, we get
	\[
	\mathbb{E}(v_{0}) = \frac{1}{D} \sum_{i=0}^{D-1} \mathbb{E}(v_{i}^2)= \frac{1}{D} \underbrace{\mathbb{E}\left(\sum_{i=0}^{D-1} v_{i}^2\right)}_{=:c^2}=\frac{c^2}{D}.
	\]
	Hence variance of the distribution of the cosine similarity is $\sigma^2=\frac{c^2}{D}$.
	In the particular case of the unit sphere $\mathbb{S}^{D-1}$, we get $c^{2}=1$ and a variance of $\sigma^2=\frac{c^2}{D}$.\\
	To see the convergence behaviour of the standardized distribution, we look at the logarithm of its density function %$f_{\phi(v,w)}\left(\frac{x}{\sqrt{D}}\right)$
	\[
	\log\left(f_{\phi(v,w)}\left(\frac{x}{\sqrt{D}}\right)\right) = \frac{D-3}{2}\log\left(1-\frac{x^2}{D}\right) + C.
	\]
	Using the Tayler series approximation of the logarithm, this transforms to
	\begin{align*}
	\log\left(f_{\phi(v,w)}\left(\frac{x}{\sqrt{D}}\right)\right) &= \frac{D-3}{2}\left(-\frac{x^2}{D} + \frac{x^4}{4D} \pm \ldots \right) + C = -\frac{1}{2}x^2 + \frac{3}{2D}x^2 + \mathcal{O}\left(\frac{x^4}{D}\right)  + C  \\
																  &\longrightarrow -\frac{1}{2}x^2 + C = \log\left(f_{\mathcal{N}}\left(x\right)\right) \textrm{ for } D \longrightarrow \infty.
	\end{align*}
	Hence, with growing $D$ the standardized distribution of the cosine similarity trends to a normal distribution.
\end{proof}